# -*- coding: utf-8 -*-
"""Neuro240 Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10HhL-D3TnTTKNd9XcznUHzbYcBw16MNl
"""

import numpy as np
import pandas as pd
import yfinance as yf
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from dataclasses import dataclass
from typing import List, Dict, Optional
import torch
import torch.nn as nn
from bindsnet.network import Network
from bindsnet.network.nodes import Input, LIFNodes, AdaptiveLIFNodes
from bindsnet.network.topology import Connection
from bindsnet.learning import PostPre, MSTDP
from bindsnet.network.monitors import Monitor
from bindsnet.analysis.plotting import plot_spikes, plot_voltages
from datetime import datetime
import random
import os
import seaborn as sns
from sklearn.model_selection import train_test_split, KFold
from bindsnet.network.nodes import Input, LIFNodes, AdaptiveLIFNodes
from bindsnet.network.topology import Connection
from bindsnet.learning import MSTDP, PostPre
from bindsnet.analysis.plotting import plot_spikes, plot_voltages
from typing import List, Tuple, Dict
from dataclasses import dataclass
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader
import matplotlib.gridspec as gridspec
from torch import tensor

@dataclass
class TradePosition:
    entry_price: float
    entry_date: datetime
    position_type: str  # 'long' or 'short'
    size: int
    exit_price: float = None
    exit_date: datetime = None
    pnl: float = None

class PortfolioMetrics:
    """
    Class to track portfolio performance metrics
    """
    def __init__(self, initial_capital=100000.0):
        self.initial_capital = initial_capital
        self.current_capital = initial_capital
        self.current_position = None
        self.trades = []
        self.daily_capital = []
        self.daily_returns = []
        self.trade_count = 0
        self.winning_trades = 0
        self.total_pnl = 0.0
    
    def open_position(self, price, date, position_type, size=1.0):
        """
        Open a new trading position
        
        Args:
            price (float): Entry price
            date (datetime): Entry date
            position_type (str): Either 'long' or 'short'
            size (float): Position size (default=1.0)
        """
        if self.current_position is not None:
            self.close_position(price, date)
        
        self.current_position = TradePosition(
            entry_price=price,
            entry_date=date,
            position_type=position_type,
            size=size
        )
    
    def close_position(self, price, date):
        """
        Close the current trading position
        
        Args:
            price (float): Exit price
            date (datetime): Exit date
        """
        if self.current_position is None:
            return
        
        # Calculate PnL
        if self.current_position.position_type == 'long':
            pnl = (price - self.current_position.entry_price) * self.current_position.size
        else:  # short
            pnl = (self.current_position.entry_price - price) * self.current_position.size
        
        # Update metrics
        self.current_capital += pnl
        self.total_pnl += pnl
        self.trade_count += 1
        if pnl > 0:
            self.winning_trades += 1
        
        # Record trade
        self.trades.append({
            'entry_date': self.current_position.entry_date,
            'exit_date': date,
            'entry_price': self.current_position.entry_price,
            'exit_price': price,
            'position_type': self.current_position.position_type,
            'pnl': pnl,
            'return': pnl / self.current_position.entry_price * 100
        })
        
        self.current_position = None
    
    def calculate_metrics(self):
        """
        Calculate portfolio performance metrics
        
        Returns:
            dict: Dictionary of performance metrics
        """
        if not self.trades:
            return {
                'total_return': 0.0,
                'win_rate': 0.0,
                'avg_return_per_trade': 0.0,
                'sharpe_ratio': 0.0,
                'max_drawdown': 0.0
            }
        
        # Calculate metrics
        total_return = (self.current_capital - self.initial_capital) / self.initial_capital * 100
        win_rate = self.winning_trades / self.trade_count * 100 if self.trade_count > 0 else 0
        avg_return_per_trade = np.mean([trade['return'] for trade in self.trades])
        
        # Calculate Sharpe ratio (assuming risk-free rate = 0)
        if len(self.daily_returns) > 1:
            returns_std = np.std(self.daily_returns)
            sharpe_ratio = np.mean(self.daily_returns) / returns_std * np.sqrt(252) if returns_std > 0 else 0
        else:
            sharpe_ratio = 0
        
        # Calculate maximum drawdown
        if len(self.daily_capital) > 0:
            cummax = np.maximum.accumulate(self.daily_capital)
            drawdown = (cummax - self.daily_capital) / cummax
            max_drawdown = np.max(drawdown) * 100 if len(drawdown) > 0 else 0
        else:
            max_drawdown = 0
        
        return {
            'total_return': total_return,
            'win_rate': win_rate,
            'avg_return_per_trade': avg_return_per_trade,
            'sharpe_ratio': sharpe_ratio,
            'max_drawdown': max_drawdown
        }

def get_stock_data(ticker, start_date, end_date):
    """
    Get historical stock data from Yahoo Finance
    
    Args:
        ticker (str): Stock ticker symbol
        start_date (str): Start date in YYYY-MM-DD format
        end_date (str): End date in YYYY-MM-DD format
        
    Returns:
        pd.DataFrame: DataFrame with OHLCV data
    """
    # Download data from Yahoo Finance
    data = yf.download(ticker, start=start_date, end=end_date)
    
    # Reset index to make Date a column
    data = data.reset_index()
    
    # Fix column names for MultiIndex columns
    if isinstance(data.columns, pd.MultiIndex):
        # Get the first level of column names (Open, High, Low, Close, Volume)
        cols = data.columns.get_level_values(0)
        # Rename columns
        data.columns = ['Date' if col == 'Date' else f'{col}_{ticker}' for col in cols]
    else:
        # Handle single-level columns
        data.columns = [col if col == 'Date' else f'{col}_{ticker}' for col in data.columns]
    
    return data

def generate_sentiment_data(stock_data):
    """
    Generate simulated sentiment data
    
    Args:
        stock_data (pd.DataFrame): Stock data with dates
        
    Returns:
        pd.DataFrame: DataFrame with dates and sentiment scores
    """
    dates = stock_data['Date']
    n = len(dates)
    
    # Generate random sentiment scores with some autocorrelation
    ar_params = [0.7]  # AR(1) parameter
    ma_params = [0.2]  # MA(1) parameter
    ar = np.random.normal(0, 1, n)
    
    # Apply AR process
    for i in range(1, n):
        ar[i] += ar_params[0] * ar[i-1]
    
    # Apply MA process
    ma = np.random.normal(0, 0.5, n)
    for i in range(1, n):
        ar[i] += ma_params[0] * ma[i-1]
    
    # Normalize to mean 0, std 1
    sentiment_scores = (ar - np.mean(ar)) / np.std(ar)
    
    # Create DataFrame with proper dates
    sentiment_data = pd.DataFrame({
        'Date': dates,
        'sentiment_score': sentiment_scores
    })
    
    return sentiment_data

def calculate_technical_indicators(data):
    """
    Calculate technical indicators for the dataset
    
    Args:
        data (pd.DataFrame): Input data with OHLCV columns
        
    Returns:
        pd.DataFrame: Data with technical indicators added
    """
    df = data.copy()
    
    # Simple Moving Averages
    df['SMA_20'] = df['Close_AAPL'].rolling(window=20).mean()
    df['SMA_50'] = df['Close_AAPL'].rolling(window=50).mean()
    
    # Exponential Moving Averages
    df['EMA_20'] = df['Close_AAPL'].ewm(span=20, adjust=False).mean()
    df['EMA_50'] = df['Close_AAPL'].ewm(span=50, adjust=False).mean()
    
    # Relative Strength Index (RSI)
    delta = df['Close_AAPL'].diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
    rs = gain / loss
    df['RSI'] = 100 - (100 / (1 + rs))
    
    # MACD
    exp1 = df['Close_AAPL'].ewm(span=12, adjust=False).mean()
    exp2 = df['Close_AAPL'].ewm(span=26, adjust=False).mean()
    df['MACD'] = exp1 - exp2
    df['MACD_signal'] = df['MACD'].ewm(span=9, adjust=False).mean()
    df['MACD_hist'] = df['MACD'] - df['MACD_signal']
    
    # Bollinger Bands
    df['BB_middle'] = df['Close_AAPL'].rolling(window=20).mean()
    bb_std = df['Close_AAPL'].rolling(window=20).std()
    df['BB_upper'] = df['BB_middle'] + (bb_std * 2)
    df['BB_lower'] = df['BB_middle'] - (bb_std * 2)
    
    # Handle NaN values
    df = df.fillna(method='bfill')
    
    return df

def calculate_atr(data, period=14):
    """Calculate Average True Range"""
    high = data['High_AAPL']
    low = data['Low_AAPL']
    close = data['Close_AAPL']
    
    tr1 = high - low
    tr2 = abs(high - close.shift(1))
    tr3 = abs(low - close.shift(1))
    
    tr = pd.DataFrame({'tr1': tr1, 'tr2': tr2, 'tr3': tr3}).max(axis=1)
    atr = tr.rolling(window=period).mean()
    
    return atr

def calculate_bollinger_bands(data, period=20, std_dev=2):
    """Calculate Bollinger Bands"""
    sma = data['Close_AAPL'].rolling(window=period).mean()
    std = data['Close_AAPL'].rolling(window=period).std()
    
    upper_band = sma + (std * std_dev)
    lower_band = sma - (std * std_dev)
    
    return upper_band, lower_band

def calculate_rsi(data, period=14):
    """Calculate Relative Strength Index"""
    delta = data['Close_AAPL'].diff()
    
    gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
    
    rs = gain / loss
    rsi = 100 - (100 / (1 + rs))
    
    return rsi

def calculate_macd(data, fast=12, slow=26, signal=9):
    """Calculate MACD (Moving Average Convergence Divergence)"""
    ema_fast = data['Close_AAPL'].ewm(span=fast, adjust=False).mean()
    ema_slow = data['Close_AAPL'].ewm(span=slow, adjust=False).mean()
    
    macd = ema_fast - ema_slow
    macd_signal = macd.ewm(span=signal, adjust=False).mean()
    
    return macd, macd_signal

def prepare_dataset(stock_data, sentiment_data, window_size=20):
    """
    Prepare dataset for training
    
    Args:
        stock_data (pd.DataFrame): Stock price data
        sentiment_data (pd.DataFrame): Sentiment data
        window_size (int): Size of the lookback window
        
    Returns:
        tuple: (X, y) where X is the features and y is the labels
    """
    # Merge stock and sentiment data
    data = pd.merge(stock_data, sentiment_data, on='Date', how='inner')
    
    # Calculate technical indicators
    data = calculate_technical_indicators(data)
    
    # Drop rows with NaN values
    data = data.dropna()
    
    # Select features
    feature_columns = [
        'Close_AAPL', 'Volume_AAPL', 'sentiment_score',
        'SMA_20', 'SMA_50', 'EMA_20', 'EMA_50',
        'RSI', 'MACD', 'MACD_signal', 'MACD_hist',
        'BB_upper', 'BB_middle', 'BB_lower'
    ]
    
    # Create sequences
    X, y = [], []
    for i in range(len(data) - window_size):
        # Get window of features
        window = data[feature_columns].iloc[i:i+window_size].values
        
        # Check for NaN values in window
        if np.isnan(window).any():
            continue
            
        # Get target (1 if price goes up, 0 if down)
        target = 1 if data['Close_AAPL'].iloc[i+window_size] > data['Close_AAPL'].iloc[i+window_size-1] else 0
        
        X.append(window)
        y.append(target)
    
    # Convert to numpy arrays
    X = np.array(X)
    y = np.array(y)
    
    # Validate data
    if np.isnan(X).any():
        raise ValueError("Features contain NaN values after preparation")
    if np.isnan(y).any():
        raise ValueError("Labels contain NaN values after preparation")
    
    return X, y

class TradingSNN(Network):
    """
    Trading model using a Spiking Neural Network with LIF neurons and STDP learning
    """
    def __init__(self, input_size, hidden_size=64, output_size=1):
        """
        Initialize the SNN model with LIF neurons and STDP learning
        
        Args:
            input_size (tuple): Input dimensions (seq_len, n_features)
            hidden_size (int): Number of hidden neurons
            output_size (int): Number of output neurons (1 for binary classification)
        """
        super(TradingSNN, self).__init__()
        
        self.seq_len, self.n_features = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size
        
        # Calculate flattened input size
        self.flat_size = self.seq_len * self.n_features
        
        # Create layers with LIF neurons
        input_layer = Input(n=self.flat_size)
        hidden_layer = LIFNodes(n=hidden_size, traces=True)  # Enable spike traces
        output_layer = LIFNodes(n=output_size, traces=True)  # Enable spike traces
        
        # Add layers to the network
        self.add_layer(input_layer, name='Input')
        self.add_layer(hidden_layer, name='Hidden')
        self.add_layer(output_layer, name='Output')
        
        # Create connections with STDP learning
        self.add_connection(Connection(source=input_layer, target=hidden_layer, w=tensor(0.05).repeat(self.flat_size, hidden_size)), source='Input', target='Hidden')
        self.add_connection(Connection(source=hidden_layer, target=output_layer, w=tensor(0.05).repeat(hidden_size, output_size)), source='Hidden', target='Output')
        
        # Add STDP learning rule
        self.add_connection(Connection(source=hidden_layer, target=output_layer, w=tensor(0.05).repeat(hidden_size, output_size), update_rule=PostPre, nu=0.01), source='Hidden', target='Output')
        
        # Monitor spikes and voltages
        self.add_monitor(Monitor(obj=hidden_layer, state_vars=['s', 'v'], time=1), name='HiddenMonitor')
        self.add_monitor(Monitor(obj=output_layer, state_vars=['s', 'v'], time=1), name='OutputMonitor')
    
    def prepare_input(self, X):
        """
        Prepare input data for the network
        
        Args:
            X (np.ndarray): Input features of shape (n_samples, seq_len, n_features)
            
        Returns:
            torch.Tensor: Prepared input tensor
        """
        # Ensure data is numeric and handle NaN values
        X = np.nan_to_num(X).astype(np.float32)
        
        # Convert to torch tensor
        X = torch.FloatTensor(X)
        
        # Normalize features
        mean = X.mean(dim=(0, 1), keepdim=True)
        std = X.std(dim=(0, 1), keepdim=True)
        X = (X - mean) / (std + 1e-8)
        
        return X
    
    def forward(self, x):
        """
        Forward pass through the network
        
        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, seq_len, n_features)
            
        Returns:
            torch.Tensor: Output spikes
        """
        # Reshape input: (batch_size, seq_len, n_features) -> (batch_size, seq_len * n_features)
        batch_size = x.size(0)
        x = x.reshape(batch_size, -1)
        
        # Run the network for a single time step
        self.run(inputs={'Input': x}, time=1)
        
        # Get output spikes
        output_spikes = self.monitors['OutputMonitor'].get('s')
        
        return output_spikes
    
    def fit(self, X_train, y_train, epochs=10, batch_size=32):
        """
        Train the network using STDP
        
        Args:
            X_train (np.ndarray): Training features
            y_train (np.ndarray): Training labels
            epochs (int): Number of training epochs
            batch_size (int): Batch size
        """
        print("\nTraining SNN model for", epochs, "epochs...")
        
        # Prepare data
        X = self.prepare_input(X_train)
        y = torch.FloatTensor(y_train)
        
        # Create dataset and dataloader
        dataset = TensorDataset(X, y.unsqueeze(1))
        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
        
        # Training mode
        self.train()
        
        for epoch in range(epochs):
            total_loss = 0
            correct = 0
            total = 0
            
            for batch_X, batch_y in dataloader:
                # Forward pass
                self.run(inputs={'Input': batch_X}, time=1)
                
                # Get output spikes
                output_spikes = self.monitors['OutputMonitor'].get('s')
                
                # Calculate loss (using spikes as output)
                loss = self.criterion(output_spikes, batch_y)
                
                # Backward pass and optimize
                self.optimizer.zero_grad()
                loss.backward()
                self.optimizer.step()
                
                # Track metrics
                total_loss += loss.item()
                predicted = (output_spikes > 0.5).float()
                total += batch_y.size(0)
                correct += (predicted == batch_y).sum().item()
            
            # Calculate epoch metrics
            avg_loss = total_loss / len(dataloader)
            accuracy = 100 * correct / total
            
            # Print epoch statistics
            print(f"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}")
    
    def predict(self, X_test):
        """
        Make predictions on test data
        
        Args:
            X_test (np.ndarray): Test features
            
        Returns:
            tuple: (predictions, confidences)
        """
        # Prepare input
        X = self.prepare_input(X_test)
        
        # Evaluation mode
        self.eval()
        
        # Make predictions
        with torch.no_grad():
            self.run(inputs={'Input': X}, time=1)
            output_spikes = self.monitors['OutputMonitor'].get('s')
        
        # Get predictions and confidences
        predictions = (output_spikes > 0.5).float().numpy().squeeze()
        confidences = output_spikes.numpy().squeeze()
        
        return predictions, confidences

def evaluate_model(model, X, y):
    """
    Evaluate the model's performance.
    
    Args:
        model: The trained model
        X (np.ndarray): Test features
        y (np.ndarray): Test labels
        
    Returns:
        dict: Dictionary containing evaluation metrics
    """
    # Make predictions
    y_pred, confidences = model.predict(X)
    
    # Calculate metrics
    metrics = {
        'accuracy': accuracy_score(y, y_pred),
        'precision': precision_score(y, y_pred),
        'recall': recall_score(y, y_pred),
        'f1': f1_score(y, y_pred),
        'avg_confidence': np.mean(confidences),
        'high_conf_accuracy': accuracy_score(y[confidences > 0.7], y_pred[confidences > 0.7]) if any(confidences > 0.7) else 0
    }
    
    return metrics

class TradingStrategy:
    """
    Trading strategy implementation with position sizing and risk management
    """
    def __init__(self, initial_capital=100000, risk_per_trade=0.02):
        self.initial_capital = initial_capital
        self.current_capital = initial_capital
        self.risk_per_trade = risk_per_trade  # Maximum risk per trade as percentage of capital
        self.position = None
        self.trades = []
        
    def calculate_position_size(self, confidence, price, stop_loss_pct=0.02):
        """
        Calculate position size based on confidence and risk management
        
        Args:
            confidence (float): Model's prediction confidence (0-1)
            price (float): Current price
            stop_loss_pct (float): Stop loss percentage
            
        Returns:
            float: Number of shares to trade
        """
        # Scale position size by confidence
        risk_amount = self.current_capital * self.risk_per_trade * confidence
        stop_loss_amount = price * stop_loss_pct
        
        # Calculate position size that risks the risk_amount given the stop loss
        position_size = risk_amount / stop_loss_amount
        
        # Round down to nearest whole share
        return int(position_size)
        
    def execute_trade(self, signal, confidence, price, timestamp):
        """
        Execute a trade based on the signal and current position
        
        Args:
            signal (int): 1 for buy, 0 for sell
            confidence (float): Model's prediction confidence
            price (float): Current price
            timestamp (pd.Timestamp): Current timestamp
            
        Returns:
            float: Profit/loss from the trade
        """
        profit = 0
        
        # Close existing position if signal differs
        if self.position is not None and signal != self.position['signal']:
            # Calculate profit/loss
            shares = self.position['shares']
            entry_price = self.position['entry_price']
            profit = shares * (price - entry_price)
            
            # Update capital
            self.current_capital += profit
            
            # Record trade
            self.trades.append({
                'entry_time': self.position['entry_time'],
                'exit_time': timestamp,
                'entry_price': entry_price,
                'exit_price': price,
                'shares': shares,
                'profit': profit,
                'return': profit / (shares * entry_price)
            })
            
            self.position = None
        
        # Open new position if none exists
        if self.position is None and confidence > 0.6:  # Only trade on high confidence
            shares = self.calculate_position_size(confidence, price)
            
            if shares > 0:
                self.position = {
                    'signal': signal,
                    'shares': shares,
                    'entry_price': price,
                    'entry_time': timestamp,
                    'stop_loss': price * (0.98 if signal == 1 else 1.02),  # 2% stop loss
                    'take_profit': price * (1.04 if signal == 1 else 0.96)  # 4% take profit
                }
        
        # Check stop loss and take profit for existing position
        elif self.position is not None:
            if (self.position['signal'] == 1 and 
                (price <= self.position['stop_loss'] or price >= self.position['take_profit'])):
                # Close long position
                profit = self.position['shares'] * (price - self.position['entry_price'])
                self.current_capital += profit
                
                self.trades.append({
                    'entry_time': self.position['entry_time'],
                    'exit_time': timestamp,
                    'entry_price': self.position['entry_price'],
                    'exit_price': price,
                    'shares': self.position['shares'],
                    'profit': profit,
                    'return': profit / (self.position['shares'] * self.position['entry_price'])
                })
                
                self.position = None
                
            elif (self.position['signal'] == 0 and 
                  (price >= self.position['stop_loss'] or price <= self.position['take_profit'])):
                # Close short position
                profit = self.position['shares'] * (self.position['entry_price'] - price)
                self.current_capital += profit
                
                self.trades.append({
                    'entry_time': self.position['entry_time'],
                    'exit_time': timestamp,
                    'entry_price': self.position['entry_price'],
                    'exit_price': price,
                    'shares': self.position['shares'],
                    'profit': profit,
                    'return': profit / (self.position['shares'] * self.position['entry_price'])
                })
                
                self.position = None
        
        return profit

def backtest_strategy(model, X_test, y_test, test_data, initial_capital=100000):
    """
    Backtest the trading strategy
    
    Args:
        model (TradingSNN): Trained model
        X_test (np.ndarray): Test features
        y_test (np.ndarray): Test labels
        test_data (pd.DataFrame): Test data with prices
        initial_capital (float): Initial capital
        
    Returns:
        tuple: (trades_df, metrics)
    """
    # Make predictions
    predictions, confidences = model.predict(X_test)
    
    # Initialize strategy
    strategy = TradingStrategy(initial_capital=initial_capital)
    
    # Run backtest
    for i in range(len(predictions)):
        price = test_data['Close_AAPL'].iloc[i]
        timestamp = test_data['Date'].iloc[i]
        
        # Execute trade
        strategy.execute_trade(predictions[i], confidences[i], price, timestamp)
    
    # Close any remaining position at the end
    if strategy.position is not None:
        final_price = test_data['Close_AAPL'].iloc[-1]
        final_timestamp = test_data['Date'].iloc[-1]
        strategy.execute_trade(1-strategy.position['signal'], 1.0, final_price, final_timestamp)
    
    # Create trades DataFrame
    trades_df = pd.DataFrame(strategy.trades)
    
    # Calculate metrics
    if len(trades_df) > 0:
        metrics = {
            'total_return': (strategy.current_capital - initial_capital) / initial_capital * 100,
            'win_rate': len(trades_df[trades_df['profit'] > 0]) / len(trades_df) * 100,
            'avg_return_per_trade': trades_df['return'].mean() * 100,
            'sharpe_ratio': trades_df['return'].mean() / trades_df['return'].std() * np.sqrt(252) if len(trades_df) > 1 else 0,
            'max_drawdown': calculate_max_drawdown(strategy.trades) * 100
        }
    else:
        metrics = {
            'total_return': 0,
            'win_rate': 0,
            'avg_return_per_trade': 0,
            'sharpe_ratio': 0,
            'max_drawdown': 0
        }
    
    return trades_df, metrics

def calculate_max_drawdown(trades):
    """
    Calculate maximum drawdown from trade history
    
    Args:
        trades (list): List of trade dictionaries
        
    Returns:
        float: Maximum drawdown as a percentage
    """
    if not trades:
        return 0
    
    # Calculate equity curve
    equity = [100000]  # Start with initial capital
    for trade in trades:
        equity.append(equity[-1] + trade['profit'])
    
    # Calculate running maximum
    running_max = np.maximum.accumulate(equity)
    
    # Calculate drawdowns
    drawdowns = (running_max - equity) / running_max
    
    return np.max(drawdowns)

def simulate_market_price(stock_data):
    """
    Simulate market prices using the enhanced market model with improved stability
    """
    # Initialize simulated prices with the first actual price
    initial_price = stock_data['Close_AAPL'].iloc[0]
    prices = [initial_price]
    
    # Get actual prices for comparison
    actual_prices = stock_data['Close_AAPL'].values
    
    # Calculate actual market volatility for calibration
    actual_returns = np.diff(actual_prices) / actual_prices[:-1]
    actual_daily_vol = np.std(actual_returns)
    
    # Simulation parameters (calibrated to actual volatility)
    base_volatility = actual_daily_vol * 0.5  # Base volatility from actual data
    momentum_factor = 0.15    # 15% momentum impact
    price_pull_factor = 0.08  # 8% pull towards actual price
    max_daily_move = actual_daily_vol * 3  # Cap at 3 standard deviations
    
    # Initialize market state
    market_sentiment = 0
    sentiment_memory = 0.8  # Memory factor for sentiment persistence
    
    # Run simulation
    for i in range(1, len(stock_data)):
        # Calculate dynamic volatility based on recent price movements
        recent_volatility = np.std(np.diff(prices[-20:]) / prices[-20:-1]) if len(prices) >= 20 else base_volatility
        current_volatility = min(base_volatility + recent_volatility * 0.5, actual_daily_vol * 1.5)
        
        # Update market sentiment with memory effect
        sentiment_impact = np.random.normal(0, 0.001)  # Small random sentiment changes
        market_sentiment = sentiment_memory * market_sentiment + (1 - sentiment_memory) * sentiment_impact
        
        # Calculate price movement components
        random_move = np.random.normal(market_sentiment, current_volatility)
        
        # Calculate momentum with bounds
        if len(prices) > 1:
            recent_return = (prices[-1] - prices[-2]) / prices[-2]
            momentum = np.clip(recent_return * momentum_factor, -actual_daily_vol, actual_daily_vol)
        else:
            momentum = 0
        
        # Calculate price gap and pull with bounds
        price_gap = (actual_prices[i] - prices[-1]) / prices[-1]
        pull = np.clip(price_gap * price_pull_factor, -actual_daily_vol, actual_daily_vol)
        
        # Combine movements and calculate new price
        total_move = np.clip(random_move + momentum + pull, -max_daily_move, max_daily_move)
        new_price = prices[-1] * (1 + total_move)
        
        # Add minimal volume impact
        volume_factor = stock_data['Volume_AAPL'].iloc[i] / stock_data['Volume_AAPL'].mean()
        volume_noise = np.random.normal(0, 0.0001 * volume_factor)
        new_price *= (1 + volume_noise)
        
        # Ensure price stays positive and within reasonable bounds
        new_price = max(0.1, min(new_price, prices[-1] * (1 + max_daily_move)))
        
        prices.append(new_price)
    
    return np.array(prices)

def plot_market_simulation(stock_data):
    """
    Plot market simulation with actual price for comparison
    """
    plt.figure(figsize=(15, 10))
    
    # Plot actual price
    plt.plot(stock_data.index, stock_data['Close_AAPL'], 
             label='Actual Price', color='black', linewidth=2)
    
    # Plot simulated price
    simulated_prices = simulate_market_price(stock_data)
    plt.plot(stock_data.index, simulated_prices, 
             label='Simulated Price', color='red', linewidth=2)
    
    plt.title('Market Price Simulation vs Actual Price')
    plt.xlabel('Date')
    plt.ylabel('Price ($)')
    plt.legend()
    plt.grid(True)
    
    plt.tight_layout()
    plt.show()

def calculate_simulation_metrics(actual_prices, simulated_prices):
    """
    Calculate key metrics comparing simulated prices to actual prices
    """
    # Ensure arrays are float type
    actual_prices = actual_prices.astype(float)
    simulated_prices = simulated_prices.astype(float)
    
    # Remove any infinite or NaN values
    mask = np.isfinite(actual_prices) & np.isfinite(simulated_prices)
    actual_prices = actual_prices[mask]
    simulated_prices = simulated_prices[mask]
    
    if len(actual_prices) == 0 or len(simulated_prices) == 0:
        print("Warning: No valid data points for metrics calculation")
        return None
    
    # Calculate metrics with error handling
    try:
        # Calculate MSE and derived metrics
        diff = actual_prices - simulated_prices
        mse = np.mean(diff ** 2)
        rmse = np.sqrt(mse) if mse > 0 else 0
        mae = np.mean(np.abs(diff))
        
        # Calculate correlation with error handling
        if np.std(actual_prices) > 0 and np.std(simulated_prices) > 0:
            correlation = np.corrcoef(actual_prices, simulated_prices)[0, 1]
        else:
            correlation = 0
        
        # Calculate directional accuracy
        actual_returns = np.diff(actual_prices)
        simulated_returns = np.diff(simulated_prices)
        valid_moves = (actual_returns != 0) & (simulated_returns != 0)
        if np.sum(valid_moves) > 0:
            directional_accuracy = np.mean(
                np.sign(actual_returns[valid_moves]) == np.sign(simulated_returns[valid_moves])
            )
        else:
            directional_accuracy = 0
        
        # Calculate volatility
        actual_volatility = np.std(actual_returns) if len(actual_returns) > 0 else 1
        simulated_volatility = np.std(simulated_returns) if len(simulated_returns) > 0 else 1
        volatility_ratio = simulated_volatility/actual_volatility if actual_volatility > 0 else 1
        
        # Print metrics
        print("\nSimulation Metrics:")
        print(f"Root Mean Square Error: ${rmse:.2f}")
        print(f"Mean Absolute Error: ${mae:.2f}")
        print(f"Correlation: {correlation:.3f}")
        print(f"Directional Accuracy: {directional_accuracy:.1%}")
        print(f"Volatility Ratio (Simulated/Actual): {volatility_ratio:.2f}")
        
        return {
            'rmse': rmse,
            'mae': mae,
            'correlation': correlation,
            'directional_accuracy': directional_accuracy,
            'volatility_ratio': volatility_ratio
        }
        
    except Exception as e:
        print(f"Error calculating metrics: {str(e)}")
        return None

def predict_next_price(current_price, recent_prices, recent_volumes, sentiment_score=None):
    """
    Predict the next price based on current market conditions
    
    Args:
        current_price (float): Current price
        recent_prices (list): List of recent prices (last 20 days)
        recent_volumes (list): List of recent volumes (last 20 days)
        sentiment_score (float, optional): Current sentiment score
        
    Returns:
        tuple: (predicted_price, confidence_interval)
    """
    # Calculate market conditions
    recent_returns = np.diff(recent_prices) / recent_prices[:-1]
    current_volatility = np.std(recent_returns) if len(recent_returns) > 0 else 0.01
    volume_factor = recent_volumes[-1] / np.mean(recent_volumes) if len(recent_volumes) > 0 else 1.0
    
    # Calculate momentum
    momentum = (recent_prices[-1] - recent_prices[0]) / recent_prices[0] if len(recent_prices) > 1 else 0
    
    # Calculate sentiment impact
    sentiment_impact = 0.001 * sentiment_score if sentiment_score is not None else 0
    
    # Calculate base price movement
    base_move = np.random.normal(0, current_volatility)
    
    # Combine factors
    total_move = base_move + 0.2 * momentum + sentiment_impact
    
    # Add volume impact
    volume_noise = np.random.normal(0, 0.0001 * volume_factor)
    total_move += volume_noise
    
    # Calculate predicted price
    predicted_price = current_price * (1 + total_move)
    
    # Calculate confidence interval (95%)
    confidence_interval = 1.96 * current_volatility * current_price
    
    return predicted_price, confidence_interval

def update_prediction_model(new_price, new_volume, sentiment_score=None):
    """
    Update the prediction model with new data
    
    Args:
        new_price (float): New price data
        new_volume (float): New volume data
        sentiment_score (float, optional): New sentiment score
    """
    # Update recent price history
    global recent_prices, recent_volumes
    recent_prices = recent_prices[1:] + [new_price]
    recent_volumes = recent_volumes[1:] + [new_volume]
    
    # Update market sentiment
    global market_sentiment
    if sentiment_score is not None:
        market_sentiment = 0.8 * market_sentiment + 0.2 * sentiment_score
    
    # Recalculate volatility
    global current_volatility
    recent_returns = np.diff(recent_prices) / recent_prices[:-1]
    current_volatility = np.std(recent_returns) if len(recent_returns) > 0 else 0.01

# Initialize global variables for real-time prediction
recent_prices = []
recent_volumes = []
market_sentiment = 0
current_volatility = 0.01

def initialize_prediction_model(initial_data, window_size=20):
    """
    Initialize the prediction model with historical data
    
    Args:
        initial_data (pd.DataFrame): Historical price and volume data
        window_size (int): Size of the lookback window
    """
    global recent_prices, recent_volumes, market_sentiment, current_volatility
    
    # Initialize price and volume history
    recent_prices = list(initial_data['Close_AAPL'].tail(window_size))
    recent_volumes = list(initial_data['Volume_AAPL'].tail(window_size))
    
    # Initialize market sentiment
    market_sentiment = 0
    
    # Calculate initial volatility
    returns = np.diff(recent_prices) / recent_prices[:-1]
    current_volatility = np.std(returns) if len(returns) > 0 else 0.01

def plot_real_time_predictions(actual_prices, predicted_prices, confidence_intervals, days_ahead=5):
    """
    Plot real-time predictions with confidence intervals
    
    Args:
        actual_prices (list): List of actual prices
        predicted_prices (list): List of predicted prices
        confidence_intervals (list): List of confidence intervals
        days_ahead (int): Number of days to predict ahead
    """
    plt.figure(figsize=(15, 8))
    
    # Create x-axis labels
    x_actual = range(len(actual_prices))
    x_pred = range(len(actual_prices), len(actual_prices) + days_ahead)
    
    # Plot actual prices
    plt.plot(x_actual, actual_prices, 'b-', label='Actual Price', linewidth=2)
    
    # Plot predicted prices with confidence intervals
    plt.plot(x_pred, predicted_prices, 'r--', label='Predicted Price', linewidth=2)
    plt.fill_between(x_pred, 
                    [p - c for p, c in zip(predicted_prices, confidence_intervals)],
                    [p + c for p, c in zip(predicted_prices, confidence_intervals)],
                    color='r', alpha=0.2, label='95% Confidence Interval')
    
    # Add vertical line at current time
    plt.axvline(x=len(actual_prices)-1, color='g', linestyle='--', label='Current Time')
    
    plt.title('Real-Time Price Predictions')
    plt.xlabel('Time (days)')
    plt.ylabel('Price ($)')
    plt.legend()
    plt.grid(True)
    
    # Add price labels
    for i, (actual, pred) in enumerate(zip(actual_prices[-5:], predicted_prices)):
        plt.annotate(f'${actual:.2f}', 
                    (len(actual_prices)-5+i, actual),
                    textcoords="offset points", xytext=(0,10), ha='center')
        plt.annotate(f'${pred:.2f}', 
                    (len(actual_prices)+i, pred),
                    textcoords="offset points", xytext=(0,10), ha='center')
    
    plt.tight_layout()
    plt.savefig('real_time_predictions.png')
    plt.close()

class SNNTrader:
    """
    Trader that uses a Spiking Neural Network to make trading decisions
    """
    def __init__(self, model, initial_capital=10000):
        self.model = model
        self.capital = initial_capital
        self.position = None

    def decide(self, market_data):
        # Select only numeric columns for prediction
        numeric_data = market_data.select_dtypes(include=[np.number])
        
        # Prepare data for prediction
        X = numeric_data[-20:].values  # Convert to NumPy array
        X = X.flatten().reshape(1, -1)  # Flatten and reshape for model input

        # Ensure the input size matches the network's expected input size
        expected_size = self.model.flat_size
        if X.shape[1] != expected_size:
            raise ValueError(f"Input size {X.shape[1]} does not match expected size {expected_size}.")

        # Make prediction
        prediction, confidence = self.model.predict(X)

        # Select the first element if prediction is multi-dimensional
        if isinstance(prediction, np.ndarray) and prediction.size > 1:
            prediction_value = prediction[0]
        else:
            prediction_value = prediction.item() if isinstance(prediction, np.ndarray) else prediction

        # Decide based on prediction
        if prediction_value > 0.5:
            return 'buy', confidence
        else:
            return 'sell', confidence

    def execute_trade(self, decision, price):
        if decision == 'buy' and self.capital > price:
            self.position = price
            self.capital -= price
        elif decision == 'sell' and self.position is not None:
            self.capital += price
            self.position = None

class SignalTrader:
    """
    Trader that uses technical signals to make trading decisions
    """
    def __init__(self, initial_capital=10000):
        self.capital = initial_capital
        self.position = None

    def decide(self, market_data):
        # Example signal: Moving Average Crossover
        short_ma = market_data['Close_AAPL'].rolling(window=5).mean().iloc[-1]
        long_ma = market_data['Close_AAPL'].rolling(window=20).mean().iloc[-1]

        if short_ma > long_ma:
            return 'buy'
        else:
            return 'sell'

    def execute_trade(self, decision, price):
        if decision == 'buy' and self.capital > price:
            self.position = price
            self.capital -= price
        elif decision == 'sell' and self.position is not None:
            self.capital += price
            self.position = None

class RandomTrader:
    """
    Trader that makes random trading decisions
    """
    def __init__(self, initial_capital=10000):
        self.capital = initial_capital
        self.position = None

    def decide(self):
        return random.choice(['buy', 'sell'])

    def execute_trade(self, decision, price):
        if decision == 'buy' and self.capital > price:
            self.position = price
            self.capital -= price
        elif decision == 'sell' and self.position is not None:
            self.capital += price
            self.position = None

def calculate_kyle_lambda(price_changes, volume):
    """
    Calculate Kyle's lambda (price impact coefficient) with enhanced accuracy
    
    Args:
        price_changes (np.array): Array of price changes
        volume (np.array): Array of trading volumes
        
    Returns:
        float: Kyle's lambda coefficient
    """
    # Convert inputs to numpy arrays if they aren't already
    price_changes = np.array(price_changes)
    volume = np.array(volume)
    
    # Ensure we have matching lengths
    min_len = min(len(price_changes), len(volume))
    if min_len == 0:
        return 0.0001  # Return default value if no data
        
    price_changes = price_changes[:min_len]
    volume = volume[:min_len]
    
    # Remove any zero volumes to avoid division by zero
    mask = volume != 0
    if not np.any(mask):
        return 0.0001
    
    price_changes = price_changes[mask]
    volume = volume[mask]
    
    # Calculate signed volume (positive for buys, negative for sells)
    signed_volume = np.sign(price_changes) * volume
    
    try:
        # Calculate lambda using covariance method with volume weighting
        weights = 1 / (volume + 1e-6)  # Add small constant to avoid division by zero
        weights = weights / np.sum(weights)  # Normalize weights
        
        # Calculate weighted covariance and variance
        mean_price_change = np.average(price_changes, weights=weights)
        mean_signed_volume = np.average(signed_volume, weights=weights)
        
        cov = np.sum(weights * (price_changes - mean_price_change) * (signed_volume - mean_signed_volume))
        var = np.sum(weights * (signed_volume - mean_signed_volume)**2)
        
        lambda_value = cov / var if var != 0 else 0.0001
        
        # Ensure the value is reasonable (not too large or small)
        lambda_value = np.clip(lambda_value, 0.00001, 0.01)
        
        return lambda_value
    except:
        return 0.0001  # Return default value if calculation fails

def calculate_amihud_illiquidity(prices, volumes, window=20):
    """
    Calculate Amihud's illiquidity measure with enhanced accuracy
    
    Args:
        prices (np.array): Array of prices
        volumes (np.array): Array of trading volumes
        window (int): Rolling window size
        
    Returns:
        np.array: Array of illiquidity measures
    """
    # Convert inputs to numpy arrays if they aren't already
    prices = np.array(prices)
    volumes = np.array(volumes)
    
    # Ensure we have enough data
    if len(prices) < 2 or len(volumes) < 2:
        return np.array([0.0001])
    
    # Calculate absolute returns with volume weighting
    returns = np.abs(np.diff(prices) / prices[:-1])
    
    # Calculate dollar volume with price impact adjustment
    dollar_volume = prices[:-1] * volumes[:-1]
    
    # Calculate illiquidity ratio with volume weighting
    illiquidity = returns / (dollar_volume + 1e-6)  # Add small constant to avoid division by zero
    
    # Apply exponential weighting to recent observations
    weights = np.exp(np.linspace(-1, 0, len(illiquidity)))
    weights = weights / np.sum(weights)
    
    # Calculate weighted rolling average
    illiquidity_ma = pd.Series(illiquidity).rolling(
        window=window,
        min_periods=1,
        win_type='gaussian'
    ).mean(std=3).fillna(0.0001)
    
    # Add market impact adjustment
    market_impact = 0.1 * np.std(returns) / np.mean(dollar_volume)
    illiquidity_ma = illiquidity_ma * (1 + market_impact)
    
    # Ensure reasonable values
    illiquidity_ma = np.clip(illiquidity_ma, 0.00001, 0.01)
    
    return illiquidity_ma.values

def calculate_market_impact(price, volume, base_volume, kyle_lambda, amihud):
    """
    Calculate market impact with enhanced accuracy
    
    Args:
        price (float): Current price
        volume (float): Current volume
        base_volume (float): Base volume (e.g., 20-day average)
        kyle_lambda (float): Kyle's lambda coefficient
        amihud (float): Amihud's illiquidity measure
        
    Returns:
        float: Market impact on price
    """
    # Calculate volume ratio
    volume_ratio = volume / base_volume
    
    # Calculate temporary impact (Kyle's lambda)
    temp_impact = kyle_lambda * np.sqrt(volume_ratio)
    
    # Calculate permanent impact (Amihud)
    perm_impact = amihud * volume_ratio
    
    # Combine impacts with volume-dependent weights
    total_impact = (
        0.7 * temp_impact +  # Temporary impact
        0.3 * perm_impact    # Permanent impact
    )
    
    # Add non-linear scaling for large trades
    if volume_ratio > 2:
        total_impact *= np.sqrt(volume_ratio)
    
    return total_impact * price

def calculate_cross_asset_correlation(returns_dict, window=20):
    """
    Calculate cross-asset correlations with dynamic adjustment
    
    Args:
        returns_dict (dict): Dictionary of return series
        window (int): Rolling window size
        
    Returns:
        pd.DataFrame: Correlation matrix
    """
    # Convert to DataFrame
    returns_df = pd.DataFrame(returns_dict)
    
    # Calculate rolling correlations
    rolling_corr = returns_df.rolling(window=window, min_periods=10).corr()
    
    # Calculate volatility ratio
    vol_ratio = returns_df.std() / returns_df.std().mean()
    
    # Adjust correlations based on volatility
    for col in returns_df.columns:
        rolling_corr.loc[col] *= (1 + 0.1 * (vol_ratio[col] - 1))
    
    # Ensure correlations are within [-1, 1]
    rolling_corr = rolling_corr.clip(-1, 1)
    
    return rolling_corr

def calculate_group_beta(returns, market_returns, window=20):
    """
    Calculate group beta with dynamic adjustment
    
    Args:
        returns (pd.Series): Group returns
        market_returns (pd.Series): Market returns
        window (int): Rolling window size
        
    Returns:
        float: Dynamic beta
    """
    # Calculate rolling covariance and variance
    rolling_cov = returns.rolling(window=window).cov(market_returns)
    rolling_var = market_returns.rolling(window=window).var()
    
    # Calculate beta
    beta = rolling_cov / rolling_var
    
    # Add momentum adjustment
    momentum = returns.pct_change(window).mean()
    beta *= (1 + 0.1 * momentum)
    
    # Ensure reasonable values
    beta = beta.clip(0.1, 3.0)
    
    return beta.fillna(1.0)

def simulate_market_multiple(traders, market_data, stock_groups, days=100):
    """
    Simulate market for multiple stocks with cross-asset interactions and improved price tracking
    """
    # Initialize results containers
    simulated_prices = {}
    simulated_volumes = {}
    group_correlations = {}
    
    # Calculate initial group betas and correlations
    group_betas = {}
    for group_name, stocks in stock_groups.items():
        group_returns = pd.DataFrame()
        for stock in stocks:
            if stock in market_data:
                returns = market_data[stock][f'Close_{stock}'].pct_change().dropna()
                group_returns[stock] = returns
        
        if not group_returns.empty:
            market_return = group_returns.mean(axis=1)
            group_betas[group_name] = group_returns.std().mean() / market_return.std()
            group_correlations[group_name] = group_returns.corr().mean().mean()
    
    # Simulate each group
    for group_name, stocks in stock_groups.items():
        group_prices = {}
        group_volumes = {}
        
        # Get group characteristics
        beta = group_betas.get(group_name, 1.0)
        correlation = group_correlations.get(group_name, 0.3)
        
        # Generate correlated market movements for the group
        market_movement = np.random.normal(0, 0.01, days)
        
        # Generate correlated random walks for the group
        random_walks = np.random.multivariate_normal(
            mean=[0] * len(stocks),
            cov=np.full((len(stocks), len(stocks)), correlation) + \
                np.eye(len(stocks)) * (1 - correlation),
            size=days
        )
        
        # Simulate each stock in the group
        for i, stock in enumerate(stocks):
            if stock in market_data:
                # Initialize with last actual price and volume
                last_price = market_data[stock][f'Close_{stock}'].iloc[-1]
                last_volume = market_data[stock][f'Volume_{stock}'].iloc[-1]
                
                # Calculate stock-specific metrics
                returns = market_data[stock][f'Close_{stock}'].pct_change().dropna()
                volatility = returns.std()
                mean_return = returns.mean()
                
                # Calculate historical price momentum and trend
                price_history = market_data[stock][f'Close_{stock}'].values[-20:]
                momentum = (price_history[-1] - price_history[0]) / price_history[0]
                
                # Calculate volume trend
                volume_history = market_data[stock][f'Volume_{stock}'].values[-20:]
                volume_trend = np.mean(volume_history[-5:]) / np.mean(volume_history)
                
                # Initialize price and volume arrays
                prices = [last_price]
                volumes = [last_volume]
                
                # Initialize stock-specific state
                momentum_factor = momentum
                stock_volatility = volatility
                volume_factor = volume_trend
                
                # Calculate Kyle's lambda and Amihud's illiquidity
                kyle_lambda = calculate_kyle_lambda(
                    np.diff(price_history) / price_history[:-1],
                    volume_history[:-1]
                )
                amihud = calculate_amihud_illiquidity(price_history, volume_history)
                
                # Calculate actual price trend using linear regression
                x = np.arange(len(price_history))
                slope, intercept = np.polyfit(x, price_history, 1)
                trend_factor = slope / price_history[-1]
                
                # Calculate directional bias from recent price action
                recent_direction = np.sign(np.diff(price_history[-5:]))
                direction_bias = np.mean(recent_direction)
                
                # Calculate price momentum using simple moving average
                price_momentum = np.mean(np.diff(price_history[-5:])) / price_history[-1]
                
                for day in range(days):
                    # Market component with reduced impact
                    market_impact = market_movement[day] * beta * (1 + 0.05 * np.sin(day/10))
                    
                    # Group component with reduced impact
                    group_impact = random_walks[day, i] * correlation * (1 + 0.03 * np.cos(day/20))
                    
                    # Stock-specific component with GARCH-like volatility
                    stock_specific = np.random.normal(mean_return, stock_volatility * 0.5)
                    
                    # Update volatility with GARCH-like effect
                    stock_volatility = np.sqrt(0.95 * stock_volatility**2 + 0.05 * stock_specific**2)
                    
                    # Enhanced momentum component with bounds
                    momentum_factor = np.clip(
                        0.95 * momentum_factor + 0.05 * (price_momentum + direction_bias),
                        -volatility * 2,
                        volatility * 2
                    )
                    
                    # Volume impact on price (reduced)
                    volume_impact = kyle_lambda * (volumes[-1] - last_volume) / last_volume * 0.5
                    
                    # Illiquidity impact (reduced)
                    illiquidity_impact = amihud[-1] * np.random.normal(0, 0.5)
                    
                    # Enhanced trend following component with bounds
                    trend_impact = np.clip(
                        trend_factor * (1 + 0.1 * np.sin(day/15)),
                        -volatility * 2,
                        volatility * 2
                    )
                    
                    # Combine all effects with adjusted weights
                    total_return = (
                        0.10 * market_impact +     # Market effect (reduced)
                        0.10 * group_impact +      # Group effect (reduced)
                        0.15 * stock_specific +    # Stock-specific effect
                        0.25 * momentum_factor +   # Momentum effect (increased)
                        0.10 * volume_impact +     # Volume impact
                        0.05 * illiquidity_impact + # Illiquidity effect
                        0.25 * trend_impact        # Trend following (increased)
                    )
                    
                    # Add directional bias with bounds
                    total_return += 0.1 * np.clip(direction_bias, -volatility, volatility)
                    
                    # Stronger mean reversion for extreme moves
                    if abs(total_return) > 1.5 * volatility:
                        total_return *= 0.6
                    
                    # Update price with total return
                    new_price = prices[-1] * (1 + total_return)
                    
                    # Tighter bounds on price movements
                    max_move = min(0.05, 0.03 + abs(momentum_factor))
                    new_price = max(prices[-1] * 0.95, min(new_price, prices[-1] * (1 + max_move)))
                    
                    # Update volume with more stable scaling
                    volume_change = np.exp(random_walks[day, i] * 0.3) * (1 + abs(total_return) * 0.5)
                    volume_change *= (1 + 0.05 * np.sin(day/5))  # Reduced cyclical component
                    new_volume = last_volume * volume_change * volume_factor
                    
                    # Update state
                    prices.append(new_price)
                    volumes.append(new_volume)
                    
                    # Slower volume factor drift
                    volume_factor = 0.98 * volume_factor + 0.02 * (new_volume / last_volume)
                
                group_prices[stock] = prices
                group_volumes[stock] = volumes
        
        simulated_prices[group_name] = group_prices
        simulated_volumes[group_name] = group_volumes
    
    return simulated_prices, simulated_volumes, group_correlations

def plot_group_results(actual_data, simulated_prices, simulated_volumes, group_name, dates):
    """
    Plot results for a group of stocks
    
    Args:
        actual_data (dict): Dictionary of actual stock data
        simulated_prices (dict): Dictionary of simulated prices
        simulated_volumes (dict): Dictionary of simulated volumes
        group_name (str): Name of the group to plot
        dates (pd.DatetimeIndex): Dates for x-axis
    """
    stocks = simulated_prices[group_name].keys()
    n_stocks = len(stocks)
    
    # Create subplot grid
    fig = plt.figure(figsize=(15, 5 * n_stocks))
    gs = gridspec.GridSpec(n_stocks, 2)
    
    for i, stock in enumerate(stocks):
        # Plot prices
        ax1 = plt.subplot(gs[i, 0])
        actual_prices = actual_data[stock][f'Close_{stock}'].values[-len(dates):]
        sim_prices = simulated_prices[group_name][stock][1:]  # Skip initial seed
        
        # Calculate price ranges for y-axis limits
        price_min = min(min(actual_prices), min(sim_prices))
        price_max = max(max(actual_prices), max(sim_prices))
        price_range = price_max - price_min
        y_min = price_min - 0.1 * price_range
        y_max = price_max + 0.1 * price_range
        
        ax1.plot(dates, actual_prices, label=f'Actual {stock}', color='blue', linewidth=2)
        ax1.plot(dates, sim_prices, label=f'Simulated {stock}', color='red', linestyle='--', linewidth=2)
        ax1.set_title(f'{stock} Price')
        ax1.set_xlabel('Date')
        ax1.set_ylabel('Price ($)')
        ax1.set_ylim(y_min, y_max)
        ax1.legend()
        ax1.grid(True)
        
        # Plot volumes
        ax2 = plt.subplot(gs[i, 1])
        actual_volumes = actual_data[stock][f'Volume_{stock}'].values[-len(dates):]
        sim_volumes = simulated_volumes[group_name][stock][1:]  # Skip initial seed
        
        # Calculate volume ranges for y-axis limits
        volume_min = min(min(actual_volumes), min(sim_volumes))
        volume_max = max(max(actual_volumes), max(sim_volumes))
        volume_range = volume_max - volume_min
        y_min = volume_min - 0.1 * volume_range
        y_max = volume_max + 0.1 * volume_range
        
        ax2.plot(dates, actual_volumes, label=f'Actual {stock}', color='blue', linewidth=2)
        ax2.plot(dates, sim_volumes, label=f'Simulated {stock}', color='red', linestyle='--', linewidth=2)
        ax2.set_title(f'{stock} Volume')
        ax2.set_xlabel('Date')
        ax2.set_ylabel('Volume')
        ax2.set_ylim(y_min, y_max)
        ax2.legend()
        ax2.grid(True)
    
    plt.tight_layout()
    plt.savefig(f'{group_name}_simulation_results.png')
    plt.close()

def analyze_group_performance(actual_data, simulated_prices, simulated_volumes, group_name):
    """
    Analyze performance metrics for a group of stocks
    
    Args:
        actual_data (dict): Dictionary of actual stock data
        simulated_prices (dict): Dictionary of simulated prices
        simulated_volumes (dict): Dictionary of simulated volumes
        group_name (str): Name of the group to analyze
        
    Returns:
        dict: Dictionary of performance metrics
    """
    metrics = {}
    stocks = simulated_prices[group_name].keys()
    
    # Calculate group-level metrics
    group_actual_returns = pd.DataFrame()
    group_sim_returns = pd.DataFrame()
    
    for stock in stocks:
        # Get actual and simulated data
        actual_prices = actual_data[stock][f'Close_{stock}'].values[-len(simulated_prices[group_name][stock])+1:]
        actual_volumes = actual_data[stock][f'Volume_{stock}'].values[-len(simulated_volumes[group_name][stock])+1:]
        sim_prices = simulated_prices[group_name][stock][1:]
        sim_volumes = simulated_volumes[group_name][stock][1:]
        
        # Calculate returns
        actual_returns = np.diff(actual_prices) / actual_prices[:-1]
        simulated_returns = np.diff(sim_prices) / sim_prices[:-1]
        
        # Store returns for group analysis
        group_actual_returns[stock] = actual_returns
        group_sim_returns[stock] = simulated_returns
        
        # Calculate individual stock metrics
        price_corr = np.corrcoef(actual_prices, sim_prices)[0,1]
        volume_corr = np.corrcoef(actual_volumes, sim_volumes)[0,1]
        
        metrics[stock] = {
            'price_correlation': price_corr,
            'r_squared': price_corr ** 2,  # Add R-squared
            'volume_correlation': volume_corr,
            'actual_volatility': np.std(actual_returns),
            'simulated_volatility': np.std(simulated_returns),
            'actual_mean_return': np.mean(actual_returns),
            'simulated_mean_return': np.mean(simulated_returns),
            'tracking_error': np.std(actual_returns - simulated_returns),
            'volume_ratio': np.mean(sim_volumes) / np.mean(actual_volumes)
        }
    
    # Calculate group-level R-squared
    group_actual_mean = group_actual_returns.mean(axis=1)
    group_sim_mean = group_sim_returns.mean(axis=1)
    group_corr = np.corrcoef(group_actual_mean, group_sim_mean)[0,1]
    group_r_squared = group_corr ** 2
    
    # Add group-level metrics
    metrics['group'] = {
        'r_squared': group_r_squared,
        'correlation': group_corr,
        'mean_r_squared': np.mean([m['r_squared'] for m in metrics.values()]),
        'median_r_squared': np.median([m['r_squared'] for m in metrics.values()])
    }
    
    return metrics

def get_sector_stocks():
    """
    Define stocks by sector for analysis
    
    Returns:
        dict: Dictionary of sectors and their constituent stocks
    """
    return {
        'Technology': ['AAPL', 'MSFT', 'NVDA', 'AMD', 'INTC'],
        'Healthcare': ['JNJ', 'PFE', 'UNH', 'ABBV', 'MRK'],
        'Finance': ['JPM', 'BAC', 'GS', 'MS', 'WFC'],
        'Consumer': ['AMZN', 'WMT', 'PG', 'KO', 'PEP'],
        'Energy': ['XOM', 'CVX', 'COP', 'SLB', 'EOG']
    }

def calculate_volatility_characteristics(stock_data):
    """
    Calculate volatility characteristics for grouping stocks
    
    Args:
        stock_data (dict): Dictionary of stock DataFrames
        
    Returns:
        dict: Dictionary with volatility metrics for each stock
    """
    volatility_metrics = {}
    
    for ticker, data in stock_data.items():
        if f'Close_{ticker}' in data.columns:
            returns = data[f'Close_{ticker}'].pct_change().dropna()
            volatility_metrics[ticker] = {
                'daily_vol': returns.std(),
                'annual_vol': returns.std() * np.sqrt(252),
                'max_drawdown': (data[f'Close_{ticker}'].cummax() - data[f'Close_{ticker}']) / data[f'Close_{ticker}'].cummax().max(),
                'beta': None  # Will be calculated later
            }
    
    return volatility_metrics

def group_stocks_by_volatility(volatility_metrics):
    """
    Group stocks by volatility characteristics
    
    Args:
        volatility_metrics (dict): Dictionary of volatility metrics
        
    Returns:
        dict: Dictionary of volatility groups and their constituent stocks
    """
    # Create DataFrame with annual volatility
    data = {'annual_vol': [metrics['annual_vol'] for metrics in volatility_metrics.values()]}
    vol_df = pd.DataFrame(data, index=volatility_metrics.keys())
    
    # Define volatility thresholds
    low_vol_threshold = vol_df['annual_vol'].quantile(0.33)
    high_vol_threshold = vol_df['annual_vol'].quantile(0.67)
    
    # Group stocks
    groups = {
        'Low_Volatility': vol_df[vol_df['annual_vol'] <= low_vol_threshold].index.tolist(),
        'Medium_Volatility': vol_df[(vol_df['annual_vol'] > low_vol_threshold) & 
                                  (vol_df['annual_vol'] <= high_vol_threshold)].index.tolist(),
        'High_Volatility': vol_df[vol_df['annual_vol'] > high_vol_threshold].index.tolist()
    }
    
    return groups

def get_market_data(start_date, end_date):
    """
    Get market data for all stocks and calculate market index
    
    Args:
        start_date (str): Start date in YYYY-MM-DD format
        end_date (str): End date in YYYY-MM-DD format
        
    Returns:
        tuple: (stock_data, market_index, sector_returns)
    """
    # Get sector stocks
    sector_stocks = get_sector_stocks()
    all_stocks = [stock for stocks in sector_stocks.values() for stock in stocks]
    
    # Download data for all stocks
    stock_data = {}
    for ticker in all_stocks:
        try:
            data = get_stock_data(ticker, start_date, end_date)
            stock_data[ticker] = data
        except Exception as e:
            print(f"Error downloading {ticker}: {str(e)}")
    
    # Calculate market index (equal-weighted)
    market_returns = pd.DataFrame()
    for ticker, data in stock_data.items():
        if f'Close_{ticker}' in data.columns:
            returns = data[f'Close_{ticker}'].pct_change()
            market_returns[ticker] = returns
    
    market_index = (1 + market_returns.mean(axis=1)).cumprod()
    
    # Calculate sector returns
    sector_returns = {}
    for sector, stocks in sector_stocks.items():
        sector_data = pd.DataFrame()
        for stock in stocks:
            if stock in stock_data and f'Close_{stock}' in stock_data[stock].columns:
                returns = stock_data[stock][f'Close_{stock}'].pct_change()
                sector_data[stock] = returns
        sector_returns[sector] = (1 + sector_data.mean(axis=1)).cumprod()
    
    return stock_data, market_index, sector_returns

def main():
    # Set date range
    start_date = '2019-01-01'
    end_date = '2024-12-31'
    
    # Get market data for all stocks
    print("Downloading market data...")
    stock_data, market_index, sector_returns = get_market_data(start_date, end_date)
    
    # Calculate volatility characteristics
    print("\nCalculating volatility characteristics...")
    volatility_metrics = calculate_volatility_characteristics(stock_data)
    
    # Group stocks by sector and volatility
    sector_groups = get_sector_stocks()
    volatility_groups = group_stocks_by_volatility(volatility_metrics)
    
    # Initialize traders
    print("\nInitializing traders...")
    traders = []
    for _ in range(100):
        numeric_cols_count = len(next(iter(stock_data.values())).columns)
        snn_model = TradingSNN(input_size=(20, numeric_cols_count))
        traders.append(SNNTrader(snn_model))
    traders.extend([SignalTrader() for _ in range(100)])
    traders.extend([RandomTrader() for _ in range(100)])
    
    # Simulate market for each group type
    print("\nSimulating market by sector...")
    sector_prices, sector_volumes, sector_correlations = simulate_market_multiple(
        traders, stock_data, sector_groups, days=100
    )
    
    print("\nSimulating market by volatility group...")
    vol_prices, vol_volumes, vol_correlations = simulate_market_multiple(
        traders, stock_data, volatility_groups, days=100
    )
    
    # Get dates for plotting
    dates = pd.date_range(end=pd.Timestamp(end_date), periods=100, freq='B')
    
    # Plot and analyze results for each sector
    print("\nAnalyzing sector performance...")
    for sector in sector_groups:
        print(f"\n{sector} Sector Analysis:")
        metrics = analyze_group_performance(stock_data, sector_prices, sector_volumes, sector)
        
        # Print group-level R-squared
        group_metrics = metrics['group']
        print(f"\nGroup-level R-squared: {group_metrics['r_squared']:.1%}")
        print(f"Mean R-squared across stocks: {group_metrics['mean_r_squared']:.1%}")
        print(f"Median R-squared across stocks: {group_metrics['median_r_squared']:.1%}")
        
        # Print individual stock metrics
        for stock, stock_metrics in metrics.items():
            if stock != 'group':  # Skip group metrics
                print(f"\n{stock} Metrics:")
                print(f"R-squared: {stock_metrics['r_squared']:.1%}")
                print(f"Price correlation: {stock_metrics['price_correlation']:.3f}")
                print(f"Volume correlation: {stock_metrics['volume_correlation']:.3f}")
                print(f"Tracking error: {stock_metrics['tracking_error']:.3f}")
                print(f"Volume ratio: {stock_metrics['volume_ratio']:.3f}")
        
        plot_group_results(stock_data, sector_prices, sector_volumes, sector, dates)
    
    # Plot and analyze results for volatility groups
    print("\nAnalyzing volatility group performance...")
    for group in volatility_groups:
        print(f"\n{group} Analysis:")
        metrics = analyze_group_performance(stock_data, vol_prices, vol_volumes, group)
        
        # Print group-level R-squared
        group_metrics = metrics['group']
        print(f"\nGroup-level R-squared: {group_metrics['r_squared']:.1%}")
        print(f"Mean R-squared across stocks: {group_metrics['mean_r_squared']:.1%}")
        print(f"Median R-squared across stocks: {group_metrics['median_r_squared']:.1%}")
        
        # Print individual stock metrics
        for stock, stock_metrics in metrics.items():
            if stock != 'group':  # Skip group metrics
                print(f"\n{stock} Metrics:")
                print(f"R-squared: {stock_metrics['r_squared']:.1%}")
                print(f"Price correlation: {stock_metrics['price_correlation']:.3f}")
                print(f"Volume correlation: {stock_metrics['volume_correlation']:.3f}")
                print(f"Tracking error: {stock_metrics['tracking_error']:.3f}")
                print(f"Volume ratio: {stock_metrics['volume_ratio']:.3f}")
        
        plot_group_results(stock_data, vol_prices, vol_volumes, group, dates)

if __name__ == "__main__":
    main()
